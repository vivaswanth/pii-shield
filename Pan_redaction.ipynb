{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6778d7fd-22e3-4255-b4d1-29b1e71c99fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc4b1fbf5394fa2b9497e81d4485114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8e64a2-1ee6-4557-a007-f2a6161d7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# -----------------------\n",
    "# 1. Setup Presidio NLP with en_core_web_md\n",
    "# -----------------------\n",
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"}],\n",
    "}\n",
    "\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(nlp_engine=nlp_engine, supported_languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e641416f-9959-4311-8628-56f11ee95796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0267c1ca759b44618e16f1e06fbef863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Redacted image saved at: pan_sreeram_pan_redacted_by_ocr2.jpg\n",
      "Donut Extract: {'docType': 'Pan', 'name': 'SREE RAMA MURTHY KATTAMURI', 'fatherName': 'SANYASI SETTY KATTAMURI', 'dob': '18 06 1977', 'docId': 'AWEPK4793N', 'side': 'Front', 'orientation': '0', 'isColoured': 'Coloured'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "# Step 1: Load Donut\n",
    "processor = DonutProcessor.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")\n",
    "\n",
    "def donut_extract(img_path):\n",
    "    # Load with PIL instead of passing string\n",
    "    pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel_values = processor(pil_img, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    outputs = model.generate(pixel_values, decoder_input_ids=decoder_input_ids, max_length=512)\n",
    "    result = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# Step 2: OCR with bounding boxes\n",
    "def run_ocr(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, lang=\"eng\")\n",
    "    return img, data\n",
    "\n",
    "# Step 3: Run Presidio\n",
    "def presidio_detect(text):\n",
    "    #analyzer = AnalyzerEngine()\n",
    "    results = analyzer.analyze(text=text, language=\"en\")\n",
    "    print(\"presidio_detect\",results);\n",
    "    return results\n",
    "\n",
    "# Step 4: Redact in image\n",
    "# def redact_image(img, ocr_data, pii_entities):\n",
    "#     for entity in pii_entities:\n",
    "#         pii_text = entity.entity_text\n",
    "#         for i, word in enumerate(ocr_data[\"text\"]):\n",
    "#             if word.strip() and re.search(re.escape(pii_text), word, re.IGNORECASE):\n",
    "#                 x, y, w, h = (\n",
    "#                     ocr_data[\"left\"][i],\n",
    "#                     ocr_data[\"top\"][i],\n",
    "#                     ocr_data[\"width\"][i],\n",
    "#                     ocr_data[\"height\"][i],\n",
    "#                 )\n",
    "#                 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # black box\n",
    "#     return img\n",
    "\n",
    "# Full pipeline\n",
    "# def redact_doc(img_path, output_path=\"redacted_donut_presidio.jpg\"):\n",
    "#     # OCR\n",
    "#     img, ocr_data = run_ocr(img_path)\n",
    "\n",
    "#     # Donut extraction (structured text)\n",
    "#     donut_result = donut_extract(img_path)\n",
    "#     print(\"donut_result\",donut_result)\n",
    "\n",
    "#     # Presidio detects PII in Donut output\n",
    "#     results = presidio_detect(donut_result)\n",
    "#     print(\"results\",results)\n",
    "\n",
    "#     # Redact in image\n",
    "#     redacted_img = redact_image(img, ocr_data, results)\n",
    "\n",
    "#     cv2.imwrite(output_path, redacted_img)\n",
    "#     return output_path, donut_result\n",
    "\n",
    "# Example usage\n",
    "# redacted_path, donut_json = redact_doc(\"final_lakshmi_aadhar.jpg\")\n",
    "# print(\"âœ… Redacted image saved at:\", redacted_path)\n",
    "# print(\"Donut Extract:\", donut_json)\n",
    "\n",
    "def parse_donut_output(raw_text):\n",
    "    # Convert tags into JSON-like dict\n",
    "    fields = re.findall(r\"<s_(.*?)>(.*?)</s_\\1>\", raw_text)\n",
    "    data = {k: v.strip() for k, v in fields if v.strip()}\n",
    "    return data\n",
    "    \n",
    "# def redact_image(img, ocr_data, pii_entities, original_text):\n",
    "#     for entity in pii_entities:\n",
    "#         # Get the actual substring from the original text\n",
    "#         pii_text = original_text[entity.start:entity.end]\n",
    "\n",
    "#         for i, word in enumerate(ocr_data[\"text\"]):\n",
    "#             if word.strip() and re.fullmatch(re.escape(pii_text.strip()), word.strip(), re.IGNORECASE):\n",
    "#                 x, y, w, h = (\n",
    "#                     ocr_data[\"left\"][i],\n",
    "#                     ocr_data[\"top\"][i],\n",
    "#                     ocr_data[\"width\"][i],\n",
    "#                     ocr_data[\"height\"][i],\n",
    "#                 )\n",
    "#                 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "#     return img\n",
    "\n",
    "# def redact_image(img, ocr_data, pii_entities):\n",
    "#     for entity in pii_entities:\n",
    "#         pii_text = entity.entity_text.strip().lower()\n",
    "\n",
    "#         for i, word in enumerate(ocr_data[\"text\"]):\n",
    "#             word_clean = word.strip().lower()\n",
    "#             if word_clean and pii_text in word_clean:\n",
    "#                 x, y, w, h = (\n",
    "#                     ocr_data[\"left\"][i],\n",
    "#                     ocr_data[\"top\"][i],\n",
    "#                     ocr_data[\"width\"][i],\n",
    "#                     ocr_data[\"height\"][i],\n",
    "#                 )\n",
    "#                 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "#     return img\n",
    "\n",
    "# def redact_image(img, ocr_data, pii_entities, original_text):\n",
    "#     for entity in pii_entities:\n",
    "#         pii_text = original_text[entity.start:entity.end].strip().lower()\n",
    "\n",
    "#         for i, word in enumerate(ocr_data[\"text\"]):\n",
    "#             word_clean = word.strip().lower()\n",
    "#             if word_clean and pii_text in word_clean:\n",
    "#                 x, y, w, h = (\n",
    "#                     ocr_data[\"left\"][i],\n",
    "#                     ocr_data[\"top\"][i],\n",
    "#                     ocr_data[\"width\"][i],\n",
    "#                     ocr_data[\"height\"][i],\n",
    "#                 )\n",
    "#                 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "#     return img\n",
    "\n",
    "def redact_image(img, ocr_data, pii_entities, original_text):\n",
    "    for entity in pii_entities:\n",
    "        # Extract the actual PII substring from the Donut text\n",
    "        pii_text = original_text[entity.start:entity.end].strip().lower()\n",
    "\n",
    "        for i, word in enumerate(ocr_data[\"text\"]):\n",
    "            word_clean = word.strip().lower()\n",
    "            if word_clean and (pii_text in word_clean or word_clean in pii_text):\n",
    "                x, y, w, h = (\n",
    "                    ocr_data[\"left\"][i],\n",
    "                    ocr_data[\"top\"][i],\n",
    "                    ocr_data[\"width\"][i],\n",
    "                    ocr_data[\"height\"][i],\n",
    "                )\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "    return img\n",
    "\n",
    "def redact_image_by_fields(img, ocr_data, pii_fields):\n",
    "    # Flatten all PII values into a list\n",
    "    pii_values = [v.strip().lower() for v in pii_fields.values() if isinstance(v, str)]\n",
    "    #print(\"pii_values\",pii_values)\n",
    "\n",
    "    for i, word in enumerate(ocr_data[\"text\"]):\n",
    "        word_clean = word.strip().lower()\n",
    "        if word_clean and any(pii in word_clean or word_clean in pii for pii in pii_values):\n",
    "            x, y, w, h = (\n",
    "                ocr_data[\"left\"][i],\n",
    "                ocr_data[\"top\"][i],\n",
    "                ocr_data[\"width\"][i],\n",
    "                ocr_data[\"height\"][i],\n",
    "            )\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "    return img\n",
    "\n",
    "def redact_doc(img_path, output_path=\"pan_sreeram_pan_redacted_by_ocr2.jpg\"):\n",
    "    # OCR\n",
    "    img, ocr_data = run_ocr(img_path)\n",
    "\n",
    "    # Donut extraction\n",
    "    donut_result = donut_extract(img_path)\n",
    "    clean_data = parse_donut_output(donut_result)\n",
    "    #print(\"Extracted PII fields:\", clean_data)\n",
    "\n",
    "    # Redact using OCR match\n",
    "    redacted_img = redact_image_by_fields(img, ocr_data, clean_data)\n",
    "\n",
    "    cv2.imwrite(output_path, redacted_img)\n",
    "    print(\"âœ… Redacted image saved at:\", output_path)\n",
    "    return output_path, clean_data\n",
    "\n",
    "# def redact_doc(img_path, output_path=\"red_pan_presidio2.jpg\"):\n",
    "#     # OCR\n",
    "#     img, ocr_data = run_ocr(img_path)\n",
    "#     print(\"ocr_data\",ocr_data);\n",
    "\n",
    "#     # Donut extraction (structured text)\n",
    "#     donut_result = donut_extract(img_path)\n",
    "#     clean_data = parse_donut_output(donut_result)\n",
    "#     print(\"clean_data\", clean_data)\n",
    "\n",
    "#     # Presidio detects PII in Donut output\n",
    "#     results = presidio_detect(donut_result)\n",
    "#     print(\"results\", results)\n",
    "\n",
    "#     # Redact in image (pass donut_result as reference text)\n",
    "#     redacted_img = redact_image(img, ocr_data, results,donut_result)\n",
    "\n",
    "#     cv2.imwrite(output_path, redacted_img)\n",
    "#     return output_path, donut_result\n",
    "# redacted_path, donut_json = redact_doc(\"pan.jpg\")\n",
    "redacted_path, donut_json = redact_doc(\"pan_sreeram.jpg\")\n",
    "#print(\"âœ… Redacted image saved at:\", redacted_path)\n",
    "print(\"Donut Extract:\", donut_json)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685d4c5d-11cc-459c-a5a0-0e76a384e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic PAN image saved at: synthetic_pan12.jpg\n",
      "Fake PAN Data: {'docType': 'Pan', 'name': 'JAMES ROBINSON', 'fatherName': 'JILLIAN ROBERTS', 'dob': '21/09/1993', 'docId': 'RGGZO6841N', 'side': 'Front', 'orientation': '0', 'isColoured': 'Coloured'}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "import string\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "def generate_fake_pan_data():\n",
    "    return {\n",
    "        \"docType\": \"Pan\",\n",
    "        \"name\": faker.name().upper(),\n",
    "        \"fatherName\": faker.name().upper(),\n",
    "        \"dob\": faker.date_of_birth(minimum_age=18, maximum_age=60).strftime(\"%d/%m/%Y\"),\n",
    "        \"docId\": ''.join(random.choices(string.ascii_uppercase, k=5)) +\n",
    "                 ''.join(random.choices(string.digits, k=4)) +\n",
    "                 random.choice(string.ascii_uppercase),\n",
    "        \"side\": \"Front\",\n",
    "        \"orientation\": \"0\",\n",
    "        \"isColoured\": \"Coloured\"\n",
    "    }\n",
    "\n",
    "def generate_fake_pan_image(template_path, output_path=\"synthetic_pan12.jpg\"):\n",
    "    fake_data = generate_fake_pan_data()\n",
    "    img = cv2.imread(template_path)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.0\n",
    "    color = (0, 0, 0)\n",
    "    thickness = 3\n",
    "\n",
    "    # Overlay text at approximate PAN card positions\n",
    "    cv2.putText(img, fake_data[\"name\"], (80, 420), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"fatherName\"], (80, 500), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"dob\"], (80, 600), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"docId\"], (300, 300), font, font_scale, color, thickness)\n",
    "\n",
    "    cv2.imwrite(output_path, img)\n",
    "    print(\"âœ… Synthetic PAN image saved at:\", output_path)\n",
    "    return output_path, fake_data\n",
    "\n",
    "template_path = \"blank_pan_template - Copy.jpg\"\n",
    "synthetic_path, fake_data = generate_fake_pan_image(template_path)\n",
    "print(\"Fake PAN Data:\", fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "419516d4-852e-404b-ac93-f9124cfb4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-37.8.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\trendsoft3\\downloads\\redaction\\docuscan\\lib\\site-packages (from faker) (2025.2)\n",
      "Downloading faker-37.8.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.3 MB/s  0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-37.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cd30f1-4bb2-4088-aaae-3223c4395d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: synthetic_pan_001.jpg\n",
      "âœ… Saved: synthetic_pan_002.jpg\n",
      "âœ… Saved: synthetic_pan_003.jpg\n",
      "âœ… Saved: synthetic_pan_004.jpg\n",
      "âœ… Saved: synthetic_pan_005.jpg\n",
      "âœ… Saved: synthetic_pan_006.jpg\n",
      "âœ… Saved: synthetic_pan_007.jpg\n",
      "âœ… Saved: synthetic_pan_008.jpg\n",
      "âœ… Saved: synthetic_pan_009.jpg\n",
      "âœ… Saved: synthetic_pan_010.jpg\n",
      "âœ… Saved: synthetic_pan_011.jpg\n",
      "âœ… Saved: synthetic_pan_012.jpg\n",
      "âœ… Saved: synthetic_pan_013.jpg\n",
      "âœ… Saved: synthetic_pan_014.jpg\n",
      "âœ… Saved: synthetic_pan_015.jpg\n",
      "âœ… Saved: synthetic_pan_016.jpg\n",
      "âœ… Saved: synthetic_pan_017.jpg\n",
      "âœ… Saved: synthetic_pan_018.jpg\n",
      "âœ… Saved: synthetic_pan_019.jpg\n",
      "âœ… Saved: synthetic_pan_020.jpg\n",
      "âœ… Saved: synthetic_pan_021.jpg\n",
      "âœ… Saved: synthetic_pan_022.jpg\n",
      "âœ… Saved: synthetic_pan_023.jpg\n",
      "âœ… Saved: synthetic_pan_024.jpg\n",
      "âœ… Saved: synthetic_pan_025.jpg\n",
      "âœ… Saved: synthetic_pan_026.jpg\n",
      "âœ… Saved: synthetic_pan_027.jpg\n",
      "âœ… Saved: synthetic_pan_028.jpg\n",
      "âœ… Saved: synthetic_pan_029.jpg\n",
      "âœ… Saved: synthetic_pan_030.jpg\n",
      "âœ… Saved: synthetic_pan_031.jpg\n",
      "âœ… Saved: synthetic_pan_032.jpg\n",
      "âœ… Saved: synthetic_pan_033.jpg\n",
      "âœ… Saved: synthetic_pan_034.jpg\n",
      "âœ… Saved: synthetic_pan_035.jpg\n",
      "âœ… Saved: synthetic_pan_036.jpg\n",
      "âœ… Saved: synthetic_pan_037.jpg\n",
      "âœ… Saved: synthetic_pan_038.jpg\n",
      "âœ… Saved: synthetic_pan_039.jpg\n",
      "âœ… Saved: synthetic_pan_040.jpg\n",
      "âœ… Saved: synthetic_pan_041.jpg\n",
      "âœ… Saved: synthetic_pan_042.jpg\n",
      "âœ… Saved: synthetic_pan_043.jpg\n",
      "âœ… Saved: synthetic_pan_044.jpg\n",
      "âœ… Saved: synthetic_pan_045.jpg\n",
      "âœ… Saved: synthetic_pan_046.jpg\n",
      "âœ… Saved: synthetic_pan_047.jpg\n",
      "âœ… Saved: synthetic_pan_048.jpg\n",
      "âœ… Saved: synthetic_pan_049.jpg\n",
      "âœ… Saved: synthetic_pan_050.jpg\n",
      "âœ… Saved: synthetic_pan_051.jpg\n",
      "âœ… Saved: synthetic_pan_052.jpg\n",
      "âœ… Saved: synthetic_pan_053.jpg\n",
      "âœ… Saved: synthetic_pan_054.jpg\n",
      "âœ… Saved: synthetic_pan_055.jpg\n",
      "âœ… Saved: synthetic_pan_056.jpg\n",
      "âœ… Saved: synthetic_pan_057.jpg\n",
      "âœ… Saved: synthetic_pan_058.jpg\n",
      "âœ… Saved: synthetic_pan_059.jpg\n",
      "âœ… Saved: synthetic_pan_060.jpg\n",
      "âœ… Saved: synthetic_pan_061.jpg\n",
      "âœ… Saved: synthetic_pan_062.jpg\n",
      "âœ… Saved: synthetic_pan_063.jpg\n",
      "âœ… Saved: synthetic_pan_064.jpg\n",
      "âœ… Saved: synthetic_pan_065.jpg\n",
      "âœ… Saved: synthetic_pan_066.jpg\n",
      "âœ… Saved: synthetic_pan_067.jpg\n",
      "âœ… Saved: synthetic_pan_068.jpg\n",
      "âœ… Saved: synthetic_pan_069.jpg\n",
      "âœ… Saved: synthetic_pan_070.jpg\n",
      "âœ… Saved: synthetic_pan_071.jpg\n",
      "âœ… Saved: synthetic_pan_072.jpg\n",
      "âœ… Saved: synthetic_pan_073.jpg\n",
      "âœ… Saved: synthetic_pan_074.jpg\n",
      "âœ… Saved: synthetic_pan_075.jpg\n",
      "âœ… Saved: synthetic_pan_076.jpg\n",
      "âœ… Saved: synthetic_pan_077.jpg\n",
      "âœ… Saved: synthetic_pan_078.jpg\n",
      "âœ… Saved: synthetic_pan_079.jpg\n",
      "âœ… Saved: synthetic_pan_080.jpg\n",
      "âœ… Saved: synthetic_pan_081.jpg\n",
      "âœ… Saved: synthetic_pan_082.jpg\n",
      "âœ… Saved: synthetic_pan_083.jpg\n",
      "âœ… Saved: synthetic_pan_084.jpg\n",
      "âœ… Saved: synthetic_pan_085.jpg\n",
      "âœ… Saved: synthetic_pan_086.jpg\n",
      "âœ… Saved: synthetic_pan_087.jpg\n",
      "âœ… Saved: synthetic_pan_088.jpg\n",
      "âœ… Saved: synthetic_pan_089.jpg\n",
      "âœ… Saved: synthetic_pan_090.jpg\n",
      "âœ… Saved: synthetic_pan_091.jpg\n",
      "âœ… Saved: synthetic_pan_092.jpg\n",
      "âœ… Saved: synthetic_pan_093.jpg\n",
      "âœ… Saved: synthetic_pan_094.jpg\n",
      "âœ… Saved: synthetic_pan_095.jpg\n",
      "âœ… Saved: synthetic_pan_096.jpg\n",
      "âœ… Saved: synthetic_pan_097.jpg\n",
      "âœ… Saved: synthetic_pan_098.jpg\n",
      "âœ… Saved: synthetic_pan_099.jpg\n",
      "âœ… Saved: synthetic_pan_100.jpg\n",
      "\n",
      "ğŸ‰ Successfully generated 100 synthetic PAN card images in 'synthetic_pan_samples' folder.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "def generate_fake_pan_data():\n",
    "    return {\n",
    "        \"docType\": \"Pan\",\n",
    "        \"name\": faker.name().upper(),\n",
    "        \"fatherName\": faker.name().upper(),\n",
    "        \"dob\": faker.date_of_birth(minimum_age=18, maximum_age=60).strftime(\"%d/%m/%Y\"),\n",
    "        \"docId\": ''.join(random.choices(string.ascii_uppercase, k=5)) +\n",
    "                 ''.join(random.choices(string.digits, k=4)) +\n",
    "                 random.choice(string.ascii_uppercase),\n",
    "        \"side\": \"Front\",\n",
    "        \"orientation\": \"0\",\n",
    "        \"isColoured\": \"Coloured\"\n",
    "    }\n",
    "\n",
    "def generate_fake_pan_image(template_path, output_path):\n",
    "    fake_data = generate_fake_pan_data()\n",
    "    img = cv2.imread(template_path)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.0\n",
    "    color = (0, 0, 0)\n",
    "    thickness = 3\n",
    "\n",
    "    # Overlay text at approximate PAN card positions\n",
    "    cv2.putText(img, fake_data[\"name\"], (80, 420), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"fatherName\"], (80, 500), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"dob\"], (80, 600), font, font_scale, color, thickness)\n",
    "    cv2.putText(img, fake_data[\"docId\"], (300, 300), font, font_scale, color, thickness)\n",
    "\n",
    "    cv2.imwrite(output_path, img)\n",
    "    return fake_data\n",
    "\n",
    "def generate_bulk_pan_images(template_path, output_dir=\"synthetic_pan_samples\", count=100):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(1, count + 1):\n",
    "        filename = f\"synthetic_pan_{i:03d}.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        fake_data = generate_fake_pan_image(template_path, output_path)\n",
    "        all_data.append(fake_data)\n",
    "        print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Successfully generated {count} synthetic PAN card images in '{output_dir}' folder.\")\n",
    "    return all_data\n",
    "\n",
    "# Run the bulk generator\n",
    "template_path = \"blank_pan_template - Copy.jpg\"\n",
    "pan_data_list = generate_bulk_pan_images(template_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a83476-f4a5-48e5-9dc7-e9e789f28e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358d2c55d7e1405d9e1577b22b201dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_001.jpg\n",
      "âœ… Redacted: synthetic_pan_001.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_002.jpg\n",
      "âœ… Redacted: synthetic_pan_002.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_003.jpg\n",
      "âœ… Redacted: synthetic_pan_003.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_004.jpg\n",
      "âœ… Redacted: synthetic_pan_004.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_005.jpg\n",
      "âœ… Redacted: synthetic_pan_005.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_006.jpg\n",
      "âœ… Redacted: synthetic_pan_006.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_007.jpg\n",
      "âœ… Redacted: synthetic_pan_007.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_008.jpg\n",
      "âœ… Redacted: synthetic_pan_008.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_009.jpg\n",
      "âœ… Redacted: synthetic_pan_009.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_010.jpg\n",
      "âœ… Redacted: synthetic_pan_010.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_011.jpg\n",
      "âœ… Redacted: synthetic_pan_011.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_012.jpg\n",
      "âœ… Redacted: synthetic_pan_012.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_013.jpg\n",
      "âœ… Redacted: synthetic_pan_013.jpg\n",
      "âœ… Redacted image saved at: redacted_pan_samples\\redacted_synthetic_pan_014.jpg\n",
      "âœ… Redacted: synthetic_pan_014.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "# Step 1: Load Donut\n",
    "processor = DonutProcessor.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"sourinkarmakar/kyc_v1-donut-demo\")\n",
    "\n",
    "def donut_extract(img_path):\n",
    "    # Load with PIL instead of passing string\n",
    "    pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel_values = processor(pil_img, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    outputs = model.generate(pixel_values, decoder_input_ids=decoder_input_ids, max_length=512)\n",
    "    result = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# Step 2: OCR with bounding boxes\n",
    "def run_ocr(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, lang=\"eng\")\n",
    "    return img, data\n",
    "\n",
    "def parse_donut_output(raw_text):\n",
    "    # Convert tags into JSON-like dict\n",
    "    fields = re.findall(r\"<s_(.*?)>(.*?)</s_\\1>\", raw_text)\n",
    "    data = {k: v.strip() for k, v in fields if v.strip()}\n",
    "    return data  \n",
    "\n",
    "def redact_image_by_fields(img, ocr_data, pii_fields):\n",
    "    # Flatten all PII values into a list\n",
    "    pii_values = [v.strip().lower() for v in pii_fields.values() if isinstance(v, str)]\n",
    "    #print(\"pii_values\",pii_values)\n",
    "\n",
    "    for i, word in enumerate(ocr_data[\"text\"]):\n",
    "        word_clean = word.strip().lower()\n",
    "        if word_clean and any(pii in word_clean or word_clean in pii for pii in pii_values):\n",
    "            x, y, w, h = (\n",
    "                ocr_data[\"left\"][i],\n",
    "                ocr_data[\"top\"][i],\n",
    "                ocr_data[\"width\"][i],\n",
    "                ocr_data[\"height\"][i],\n",
    "            )\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 0), -1)  # blackout\n",
    "    return img    \n",
    "def redact_doc(img_path, output_path=\"pan_sreeram_pan_redacted_by_ocr2.jpg\"):\n",
    "    # OCR\n",
    "    img, ocr_data = run_ocr(img_path)\n",
    "\n",
    "    # Donut extraction\n",
    "    donut_result = donut_extract(img_path)\n",
    "    clean_data = parse_donut_output(donut_result)\n",
    "    #print(\"Extracted PII fields:\", clean_data)\n",
    "\n",
    "    # Redact using OCR match\n",
    "    redacted_img = redact_image_by_fields(img, ocr_data, clean_data)\n",
    "\n",
    "    cv2.imwrite(output_path, redacted_img)\n",
    "    print(\"âœ… Redacted image saved at:\", output_path)\n",
    "    return output_path, clean_data\n",
    "\n",
    "def batch_redact_pan_images(input_dir=\"synthetic_pan_samples\", output_dir=\"redacted_pan_samples\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    redacted_data = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, f\"redacted_{filename}\")\n",
    "            \n",
    "            try:\n",
    "                redacted_img_path, extracted_data = redact_doc(input_path, output_path)\n",
    "                redacted_data.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"redacted_path\": redacted_img_path,\n",
    "                    \"extracted_data\": extracted_data\n",
    "                })\n",
    "                print(f\"âœ… Redacted: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to redact {filename}: {e}\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Completed redaction for {len(redacted_data)} PAN images.\")\n",
    "    return redacted_data\n",
    "\n",
    "# Run the batch redaction\n",
    "redacted_results = batch_redact_pan_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0b7cc-958f-4f11-9736-23fa5b3d5886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
